"""
Evaluation Metrics cho RAG System
Bao gá»“m: Precision, Recall, F1, MRR, NDCG, Hit Rate
"""
import numpy as np
from typing import List, Dict, Tuple


class RAGMetrics:
    """Metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng Retrieval-Augmented Generation"""

    @staticmethod
    def precision_at_k(retrieved: List[str], relevant: List[str], k: int = 5) -> float:
        """
        Precision@K: Tá»· lá»‡ documents liÃªn quan trong top-K káº¿t quáº£

        Args:
            retrieved: Danh sÃ¡ch documents Ä‘Æ°á»£c retrieve
            relevant: Danh sÃ¡ch documents thá»±c sá»± liÃªn quan
            k: Sá»‘ lÆ°á»£ng top results Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

        Returns:
            Precision score (0-1)
        """
        if not retrieved or k == 0:
            return 0.0

        top_k = retrieved[:k]
        relevant_set = set(relevant)

        hits = sum(1 for doc in top_k if doc in relevant_set)
        return hits / k

    @staticmethod
    def recall_at_k(retrieved: List[str], relevant: List[str], k: int = 5) -> float:
        """
        Recall@K: Tá»· lá»‡ documents liÃªn quan Ä‘Æ°á»£c tÃ¬m tháº¥y trong top-K

        Returns:
            Recall score (0-1)
        """
        if not relevant:
            return 0.0

        top_k = retrieved[:k]
        relevant_set = set(relevant)

        hits = sum(1 for doc in top_k if doc in relevant_set)
        return hits / len(relevant_set)

    @staticmethod
    def f1_at_k(retrieved: List[str], relevant: List[str], k: int = 5) -> float:
        """
        F1@K: Harmonic mean cá»§a Precision vÃ  Recall

        Returns:
            F1 score (0-1)
        """
        precision = RAGMetrics.precision_at_k(retrieved, relevant, k)
        recall = RAGMetrics.recall_at_k(retrieved, relevant, k)

        if precision + recall == 0:
            return 0.0

        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def mean_reciprocal_rank(retrieved: List[str], relevant: List[str]) -> float:
        """
        MRR (Mean Reciprocal Rank): Vá»‹ trÃ­ cá»§a document liÃªn quan Ä‘áº§u tiÃªn

        MRR = 1/rank cá»§a káº¿t quáº£ Ä‘Ãºng Ä‘áº§u tiÃªn

        Returns:
            MRR score (0-1)
        """
        relevant_set = set(relevant)

        for rank, doc in enumerate(retrieved, start=1):
            if doc in relevant_set:
                return 1.0 / rank

        return 0.0

    @staticmethod
    def ndcg_at_k(retrieved: List[str], relevant: List[str], k: int = 5) -> float:
        """
        NDCG@K (Normalized Discounted Cumulative Gain)
        ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng ranking, documents liÃªn quan á»Ÿ vá»‹ trÃ­ cao hÆ¡n Ä‘Æ°á»£c thÆ°á»Ÿng nhiá»u hÆ¡n

        Returns:
            NDCG score (0-1)
        """
        if not relevant:
            return 0.0

        top_k = retrieved[:k]
        relevant_set = set(relevant)

        # DCG: TÃ­nh gain vá»›i discount theo vá»‹ trÃ­
        dcg = 0.0
        for rank, doc in enumerate(top_k, start=1):
            if doc in relevant_set:
                # Gain = 1 náº¿u relevant, 0 náº¿u khÃ´ng
                gain = 1.0
                # Discount theo log2(rank + 1)
                dcg += gain / np.log2(rank + 1)

        # IDCG: DCG lÃ½ tÆ°á»Ÿng (táº¥t cáº£ relevant docs á»Ÿ Ä‘áº§u)
        ideal_length = min(len(relevant), k)
        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_length + 1))

        if idcg == 0:
            return 0.0

        return dcg / idcg

    @staticmethod
    def hit_rate_at_k(retrieved: List[str], relevant: List[str], k: int = 5) -> float:
        """
        Hit Rate@K: CÃ³ Ã­t nháº¥t 1 document liÃªn quan trong top-K khÃ´ng?

        Returns:
            1.0 náº¿u cÃ³ hit, 0.0 náº¿u khÃ´ng
        """
        top_k = retrieved[:k]
        relevant_set = set(relevant)

        for doc in top_k:
            if doc in relevant_set:
                return 1.0

        return 0.0

    @staticmethod
    def evaluate_retrieval(
        retrieved_docs: List[List[str]],
        relevant_docs: List[List[str]],
        k_values: List[int] = [1, 3, 5, 10]
    ) -> Dict[str, Dict[int, float]]:
        """
        ÄÃ¡nh giÃ¡ toÃ n diá»‡n retrieval system vá»›i nhiá»u queries

        Args:
            retrieved_docs: List cÃ¡c retrieved documents cho má»—i query
            relevant_docs: List cÃ¡c relevant documents cho má»—i query
            k_values: CÃ¡c giÃ¡ trá»‹ K Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

        Returns:
            Dictionary chá»©a táº¥t cáº£ metrics
        """
        results = {
            'precision': {},
            'recall': {},
            'f1': {},
            'ndcg': {},
            'hit_rate': {},
            'mrr': 0.0
        }

        n_queries = len(retrieved_docs)
        mrr_sum = 0.0

        for k in k_values:
            precision_sum = 0.0
            recall_sum = 0.0
            f1_sum = 0.0
            ndcg_sum = 0.0
            hit_sum = 0.0

            for retrieved, relevant in zip(retrieved_docs, relevant_docs):
                precision_sum += RAGMetrics.precision_at_k(retrieved, relevant, k)
                recall_sum += RAGMetrics.recall_at_k(retrieved, relevant, k)
                f1_sum += RAGMetrics.f1_at_k(retrieved, relevant, k)
                ndcg_sum += RAGMetrics.ndcg_at_k(retrieved, relevant, k)
                hit_sum += RAGMetrics.hit_rate_at_k(retrieved, relevant, k)

            results['precision'][k] = precision_sum / n_queries
            results['recall'][k] = recall_sum / n_queries
            results['f1'][k] = f1_sum / n_queries
            results['ndcg'][k] = ndcg_sum / n_queries
            results['hit_rate'][k] = hit_sum / n_queries

        # MRR tÃ­nh riÃªng (khÃ´ng phá»¥ thuá»™c vÃ o K)
        for retrieved, relevant in zip(retrieved_docs, relevant_docs):
            mrr_sum += RAGMetrics.mean_reciprocal_rank(retrieved, relevant)

        results['mrr'] = mrr_sum / n_queries

        return results


def print_evaluation_results(results: Dict, model_name: str = "Model"):
    """In káº¿t quáº£ Ä‘Ã¡nh giÃ¡ dá»… Ä‘á»c"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ: {model_name}")
    print(f"{'='*60}")

    print(f"\nğŸ¯ MRR (Mean Reciprocal Rank): {results['mrr']:.4f}")

    print(f"\nğŸ“ˆ Metrics theo K:")
    print(f"{'Metric':<15} {'K=1':<10} {'K=3':<10} {'K=5':<10} {'K=10':<10}")
    print("-" * 60)

    for metric in ['precision', 'recall', 'f1', 'ndcg', 'hit_rate']:
        if metric in results and isinstance(results[metric], dict):
            values = results[metric]
            row = f"{metric.upper():<15}"
            for k in [1, 3, 5, 10]:
                if k in values:
                    row += f"{values[k]:<10.4f}"
                else:
                    row += f"{'N/A':<10}"
            print(row)

    print("=" * 60)
