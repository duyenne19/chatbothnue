# rag/rag_chatbot.py
import os
from dotenv import load_dotenv
load_dotenv()

import google.generativeai as genai

from rag.config import RAGConfig
from rag.markdown_loader import MarkdownLoader
from rag.text_chunker import TextChunker
from rag.vector_store import VectorStore


class RAGChatbot:
    """
    RAG Chatbot tuy·ªÉn sinh
    - Ch·ªâ CHAT
    - KH√îNG crawl
    - Ch·ªâ d√πng d·ªØ li·ªáu Markdown ƒë√£ chu·∫©n b·ªã s·∫µn
    """

    def __init__(self, config: RAGConfig):
        self.config = config

        # ===== Load Gemini API =====
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise RuntimeError(
                "‚ùå Thi·∫øu GEMINI_API_KEY. "
                "H√£y c·∫•u h√¨nh trong file .env"
            )

        genai.configure(api_key=api_key)

        # C·∫•u h√¨nh generation ƒë·ªÉ c√¢u tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
        generation_config = genai.GenerationConfig(
            temperature=0.7,  # V·ª´a ƒë·ªß creative nh∆∞ng v·∫´n ch√≠nh x√°c
            top_p=0.9,
            top_k=40,
            max_output_tokens=1024,
        )

        # Safety settings ƒë·ªÉ tr√°nh b·ªã block
        safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_NONE",
            },
        ]

        self.model = genai.GenerativeModel(
            config.gemini_model,
            generation_config=generation_config,
            safety_settings=safety_settings
        )

        # ===== RAG components =====
        self.loader = MarkdownLoader(config.markdown_dir)
        self.chunker = TextChunker(
            size=config.chunk_size,
            overlap=config.overlap
        )
        self.store = VectorStore(config.embedding_model)

        self.ready = False

    # -------------------------------------------------
    def initialize(self):
        """
        Kh·ªüi t·∫°o RAG:
        - Load Markdown
        - Chunk
        - Build vector store
        """
        print("üìÑ ƒêang load d·ªØ li·ªáu Markdown...")

        documents = self.loader.load()
        if not documents:
            raise RuntimeError(
                "‚ùå Kh√¥ng t√¨m th·∫•y file content.md.\n"
                "üëâ H√£y ch·∫°y crawler tr∆∞·ªõc ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu."
            )

        chunks = []
        for doc in documents:
            doc_chunks = self.chunker.chunk(doc)
            chunks.extend(doc_chunks)

        if not chunks:
            raise RuntimeError(
                "‚ùå D·ªØ li·ªáu kh√¥ng ƒë·ªß ƒë·ªÉ t·∫°o chunk."
            )

        print(f"üß© T·ªïng s·ªë chunk: {len(chunks)}")
        self.store.build(chunks)

        self.ready = True
        print("ü§ñ Chatbot s·∫µn s√†ng!")

    # -------------------------------------------------
    def ask(self, question: str) -> str:
        """
        Tr·∫£ l·ªùi c√¢u h·ªèi ng∆∞·ªùi d√πng
        """
        if not self.ready:
            return "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng."

        if not question.strip():
            return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi."

        # ===== Retrieve =====
        contexts = self.store.search(
            question,
            self.config.top_k
        )

        if not contexts:
            return "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong d·ªØ li·ªáu tuy·ªÉn sinh."

        context_text = "\n\n".join(contexts)

        # ===== Prompt =====
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n tuy·ªÉn sinh th√¥ng minh c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc S∆∞ ph·∫°m H√† N·ªôi.

NHI·ªÜM V·ª§:
- ƒê·ªçc k·ªπ ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p
- T·ªïng h·ª£p th√¥ng tin th√†nh c√¢u tr·∫£ l·ªùi T·ª∞ NHI√äN nh∆∞ ng∆∞·ªùi th·∫≠t ƒëang t∆∞ v·∫•n
- Tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp
- CH·ªà d√πng th√¥ng tin t·ª´ ng·ªØ c·∫£nh, KH√îNG b·ªãa ƒë·∫∑t

QUY T·∫ÆC B·∫ÆT BU·ªòC:
‚úó KH√îNG copy/paste nguy√™n vƒÉn t·ª´ ng·ªØ c·∫£nh
‚úó KH√îNG dump th√¥ng tin d·∫°ng bullet points
‚úó KH√îNG tr·∫£ l·ªùi chung chung
‚úì PH·∫¢I vi·∫øt th√†nh c√¢u vƒÉn t·ª± nhi√™n, m·∫°ch l·∫°c
‚úì N·∫øu h·ªèi v·ªÅ ng√†nh: gi·ªõi thi·ªáu t·ªïng quan + c∆° h·ªôi ngh·ªÅ nghi·ªáp
‚úì N·∫øu h·ªèi v·ªÅ ƒëi·ªÉm/ch·ªâ ti√™u: n√™u con s·ªë c·ª• th·ªÉ (n·∫øu c√≥)
‚úì N·∫øu kh√¥ng c√≥ th√¥ng tin: "Hi·ªán t√¥i ch∆∞a c√≥ d·ªØ li·ªáu v·ªÅ... B·∫°n c√≥ th·ªÉ li√™n h·ªá ph√≤ng tuy·ªÉn sinh ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt."

V√ç D·ª§ TR·∫¢ L·ªúI T·ªêT:
C√¢u h·ªèi: "Ng√†nh CNTT h·ªçc nh·ªØng g√¨?"
Tr·∫£ l·ªùi: "Ng√†nh C√¥ng ngh·ªá th√¥ng tin t·∫°i tr∆∞·ªùng ƒë√†o t·∫°o c√°c ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªÅ l·∫≠p tr√¨nh, c∆° s·ªü d·ªØ li·ªáu, m·∫°ng m√°y t√≠nh v√† ph√°t tri·ªÉn ph·∫ßn m·ªÅm. Sinh vi√™n s·∫Ω ƒë∆∞·ª£c h·ªçc c·∫£ l√Ω thuy·∫øt v√† th·ª±c h√†nh qua c√°c d·ª± √°n th·ª±c t·∫ø. Sau khi t·ªët nghi·ªáp, b·∫°n c√≥ th·ªÉ l√†m vi·ªác t·∫°i c√°c c√¥ng ty c√¥ng ngh·ªá, ng√¢n h√†ng, ho·∫∑c tr·ªü th√†nh gi√°o vi√™n Tin h·ªçc."

NG·ªÆ C·∫¢NH THAM KH·∫¢O:
{context_text}

C√ÇU H·ªéI:
{question}

TR·∫¢ L·ªúI (ng·∫Øn g·ªçn, t·ª± nhi√™n):"""

        # ===== Generate =====
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()

        except Exception as e:
            return f"‚ùå L·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}"
